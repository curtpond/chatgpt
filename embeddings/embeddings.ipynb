{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A on Embeddings\n",
    "OpenAI tutorial that walks through a simple example of crawling a website (in this example, the OpenAI website), turning the crawled pages into embeddings using the Embeddings API, and then creating a basic search functionality that allows a user to ask questions about the embedded information. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a webcrawler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This crawler will start from the root URL passed in at the bottom of the code below, visit each page, find additional links, and visit those pages as well (as long as they have the same root domain). To begin, import the required packages, set up the basic URL, and define a HTMLParser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to match the URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "domain = 'openai.com'\n",
    "full_url = 'https://openai.com/'\n",
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "domain = \"openai.com\" # <- put your domain to be crawled\n",
    "full_url = \"https://openai.com/\" # <- put your domain to be crawled with https or http\n",
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function takes a URL as an argument, opens the URL, and reads the HTML content. Then, it returns all the hyperlinks found on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # If the response is not HTML, return an empty list\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # Decode the HTML\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to crawl through and index only the content that lives under the OpenAI domain. For this purpose, a function that calls the get_hyperlinks function but filters out any URLs that are not part of the specified domain is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # Parse the URL and check if the domain is the same\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crawl function is the final step in the web scraping task setup. It keeps track of the visited URLs to avoid repeating the same page, which might be linked across multiple pages on a site. It also extracts the raw text from a page without the HTML tags, and writes the text content into a local .txt file specific to the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = set([url])\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "\n",
    "        # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(url) # for debugging and to see the progress\n",
    "\n",
    "        # Save text from the url to a <url>.txt file\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "\n",
    "            # Get the text from the URL using BeautifulSoup\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # Get the text but remove the tags\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # Otherwise, write the text to the file in the text directory\n",
    "            f.write(text)\n",
    "\n",
    "        # Get the hyperlinks from the URL and add them to the queue\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an Embeddings Index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV is a common format for storing embeddings. You can use this format with Python by converting the raw text files (which are in the text directory) into Pandas data frames. Pandas is a popular open source library that helps you work with tabular data (data stored in rows and columns).\n",
    "\n",
    "Blank empty lines can clutter the text files and make them harder to process. A simple function can remove those lines and tidy up the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the text to CSV requires looping through the text files in the text directory created earlier. After opening each file, remove the extra spacing and append the modified text to a list. Then, add the text with the new lines removed to an empty Pandas data frame and write the data frame to a CSV file.\n",
    "\n",
    "Extra spacing and new lines can clutter the text and complicate the embeddings process. The code used here helps to remove some of them but you may find 3rd party libraries or other methods useful to get rid of more unnecessary characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/t1w3xsh916z60p5jhmd5xbc00000gn/T/ipykernel_29569/2931785837.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  serie = serie.str.replace('\\\\n', ' ')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>research?authors=arthur petron</td>\n",
       "      <td>research?authors=arthur petron.  Research inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blog dall e 2 update#OpenAI</td>\n",
       "      <td>blog dall e 2 update#OpenAI.  DALL·E 2 researc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>research?authors=amanda askell</td>\n",
       "      <td>research?authors=amanda askell.  Research inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>research?authors=kenny hsu</td>\n",
       "      <td>research?authors=kenny hsu.  Research index   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>careers software engineer platform reliability</td>\n",
       "      <td>careers software engineer platform reliability...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            fname  \\\n",
       "0                  research?authors=arthur petron   \n",
       "1                     blog dall e 2 update#OpenAI   \n",
       "2                  research?authors=amanda askell   \n",
       "3                      research?authors=kenny hsu   \n",
       "4  careers software engineer platform reliability   \n",
       "\n",
       "                                                text  \n",
       "0  research?authors=arthur petron.  Research inde...  \n",
       "1  blog dall e 2 update#OpenAI.  DALL·E 2 researc...  \n",
       "2  research?authors=amanda askell.  Research inde...  \n",
       "3  research?authors=kenny hsu.  Research index   ...  \n",
       "4  careers software engineer platform reliability...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list to store the text files\n",
    "texts=[]\n",
    "\n",
    "# Get all the text files in the text directory\n",
    "for file in os.listdir(\"text/\" + domain + \"/\"):\n",
    "\n",
    "    # Open the file and read the text\n",
    "    with open(\"text/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n",
    "\n",
    "# Create a dataframe from the list of texts\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "# Set the text column to be the raw text with the newlines removed\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "df.to_csv('processed/scraped.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the next step after saving the raw text into a CSV file. This process splits the input text into tokens by breaking down the sentences and words. A visual demonstration of this can be seen by checking out our Tokenizer in the docs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API has a limit on the maximum number of input tokens for embeddings. To stay below the limit, the text in the CSV file needs to be broken down into multiple rows. The existing length of each row will be recorded first to identify which rows need to be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
    "df.columns = ['title', 'text']\n",
    "\n",
    "# Tokenize the text and save the number of tokens to a new column\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# Visualize the distribution of the number of tokens per row using a histogram\n",
    "df.n_tokens.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newest embeddings model can handle inputs with up to 8191 input tokens so most of the rows would not need any chunking, but this may not be the case for every subpage scraped so the next code chunk will split the longer lines into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 500\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens\n",
    "def split_into_many(text, max_tokens = max_tokens):\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of \n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "\n",
    "shortened = []\n",
    "\n",
    "# Loop through the dataframe\n",
    "for row in df.iterrows():\n",
    "\n",
    "    # If the text is None, go to the next row\n",
    "    if row[1]['text'] is None:\n",
    "        continue\n",
    "\n",
    "    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
    "    if row[1]['n_tokens'] > max_tokens:\n",
    "        shortened += split_into_many(row[1]['text'])\n",
    "    \n",
    "    # Otherwise, add the text to the list of shortened texts\n",
    "    else:\n",
    "        shortened.append( row[1]['text'] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the updated histogram again can help to confirm if the rows were successfully split into shortened sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAARC0lEQVR4nO3df6zddX3H8edrBRGtQjvkpqFk7ZLGrcjm5Ia5sZjbYUYVY/ljkBp1ZWFpsqDDjWUpM5nZH83Ykpm5MJY0YtYF502HGhoJm6R6Y0yGjCoOSu2owrDC6JyAXrOgZe/9cb5uZ/UWOOfcnsM9n+cjuTnf7/t8v9/P532a+zrffs+Pm6pCktSGn5j0BCRJ42PoS1JDDH1JaoihL0kNMfQlqSFnTHoCL+a8886rDRs2DLzf97//fV796lcv/4RexlrrubV+wZ5bsRw9Hzx48NtV9bqT6y/70N+wYQP333//wPstLCwwNze3/BN6GWut59b6BXtuxXL0nOTflqp7eUeSGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhrysv9EriRN0oZdd419zBsvPsHcaTq2Z/qS1JAXDf0kH0tyPMlDfbW1Se5J8kh3u6bvvpuSHE1yJMkVffVLkjzY3feXSbL87UiSXshLOdP/G2DrSbVdwIGq2gQc6NZJshnYDlzU7XNrklXdPn8N7AQ2dT8nH1OSdJq9aOhX1ReA75xU3gbs7Zb3Alf11eer6rmqehQ4ClyaZB3w2qr6p+r9Jfa/7dtHkjQmw76QO1NVTwJU1ZNJzu/qFwD39m13rKv9sFs+ub6kJDvp/a+AmZkZFhYWBp7g4uLiUPutZK313Fq/YM+TcOPFJ8Y+5szZnLael/vdO0tdp68XqC+pqvYAewBmZ2drmO+V9ju4p19r/YI9T8K1E3r3zjWnqedh373zVHfJhu72eFc/BlzYt9164Imuvn6JuiRpjIYN/f3Ajm55B3BnX317krOSbKT3gu193aWg7yV5c/eund/o20eSNCYvenknySeAOeC8JMeADwE3A/uSXAc8DlwNUFWHkuwDHgZOANdX1fPdoX6b3juBzgbu7n4kSWP0oqFfVe86xV2Xn2L73cDuJer3A28YaHaSpGXlJ3IlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWrISKGf5HeTHEryUJJPJHllkrVJ7knySHe7pm/7m5IcTXIkyRWjT1+SNIihQz/JBcDvALNV9QZgFbAd2AUcqKpNwIFunSSbu/svArYCtyZZNdr0JUmDGPXyzhnA2UnOAF4FPAFsA/Z29+8FruqWtwHzVfVcVT0KHAUuHXF8SdIAUlXD75zcAOwG/gv4bFW9O8kzVXVu3zZPV9WaJLcA91bV7V39NuDuqrpjiePuBHYCzMzMXDI/Pz/w3BYXF1m9evUwba1YrfXcWr9gz5Pw4LeeHfuYM2fD+WvPGekYW7ZsOVhVsyfXzxj2gN21+m3ARuAZ4O+TvOeFdlmituQzTlXtAfYAzM7O1tzc3MDzW1hYYJj9VrLWem6tX7DnSbh2111jH/PGi09wzWnqeZTLO28FHq2q/6iqHwKfAn4ZeCrJOoDu9ni3/THgwr7919O7HCRJGpNRQv9x4M1JXpUkwOXAYWA/sKPbZgdwZ7e8H9ie5KwkG4FNwH0jjC9JGtDQl3eq6ktJ7gC+DJwAvkLvksxqYF+S6+g9MVzdbX8oyT7g4W7766vq+RHnL0kawNChD1BVHwI+dFL5OXpn/Uttv5veC7+SpAnwE7mS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0JekhowU+knOTXJHkq8lOZzkl5KsTXJPkke62zV929+U5GiSI0muGH36kqRBjHqm/xHgH6rqZ4CfBw4Du4ADVbUJONCtk2QzsB24CNgK3Jpk1YjjS5IGMHToJ3kt8BbgNoCq+kFVPQNsA/Z2m+0FruqWtwHzVfVcVT0KHAUuHXZ8SdLgUlXD7Zi8EdgDPEzvLP8gcAPwrao6t2+7p6tqTZJbgHur6vaufhtwd1XdscSxdwI7AWZmZi6Zn58feH6Li4usXr164P1WstZ6bq1fsOdJePBbz459zJmz4fy154x0jC1bthysqtmT62eMcMwzgDcB76+qLyX5CN2lnFPIErUln3Gqag+9JxRmZ2drbm5u4MktLCwwzH4rWWs9t9Yv2PMkXLvrrrGPeePFJ7jmNPU8yjX9Y8CxqvpSt34HvSeBp5KsA+huj/dtf2Hf/uuBJ0YYX5I0oKFDv6r+Hfhmktd3pcvpXerZD+zoajuAO7vl/cD2JGcl2QhsAu4bdnxJ0uBGubwD8H7g40leAXwD+E16TyT7klwHPA5cDVBVh5Lso/fEcAK4vqqeH3F8SdIARgr9qnoA+LEXCuid9S+1/W5g9yhjSpKG5ydyJakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqyMihn2RVkq8k+Uy3vjbJPUke6W7X9G17U5KjSY4kuWLUsSVJg1mOM/0bgMN967uAA1W1CTjQrZNkM7AduAjYCtyaZNUyjC9JeolGCv0k64ErgY/2lbcBe7vlvcBVffX5qnquqh4FjgKXjjK+JGkwqarhd07uAP4EeA3w+1X1jiTPVNW5fds8XVVrktwC3FtVt3f124C7q+qOJY67E9gJMDMzc8n8/PzAc1tcXGT16tXDtLVitdZza/2CPU/Cg996duxjzpwN5689Z6RjbNmy5WBVzZ5cP2PYAyZ5B3C8qg4mmXspuyxRW/IZp6r2AHsAZmdna27upRz+/1tYWGCY/Vay1npurV+w50m4dtddYx/zxotPcM1p6nno0AcuA96Z5O3AK4HXJrkdeCrJuqp6Msk64Hi3/THgwr791wNPjDC+JGlAQ1/Tr6qbqmp9VW2g9wLt56rqPcB+YEe32Q7gzm55P7A9yVlJNgKbgPuGnrkkaWCjnOmfys3AviTXAY8DVwNU1aEk+4CHgRPA9VX1/GkYX5J0CssS+lW1ACx0y/8JXH6K7XYDu5djTEnS4PxEriQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTkdH86SpGW1YQLffzOtPNOXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIVP9ffqT+g7ux26+ciLjStKL8Uxfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGDB36SS5M8vkkh5McSnJDV1+b5J4kj3S3a/r2uSnJ0SRHklyxHA1Ikl66Uc70TwA3VtXPAm8Grk+yGdgFHKiqTcCBbp3uvu3ARcBW4NYkq0aZvCRpMEOHflU9WVVf7pa/BxwGLgC2AXu7zfYCV3XL24D5qnquqh4FjgKXDju+JGlwqarRD5JsAL4AvAF4vKrO7bvv6apak+QW4N6qur2r3wbcXVV3LHG8ncBOgJmZmUvm5+cHntPi4iKPPvv8EN2M7uILzpnIuIuLi6xevXoiY09Ca/1Cuz1P6nd5UmbOhvPXjpYjW7ZsOVhVsyfXR/7unSSrgU8CH6iq7yY55aZL1JZ8xqmqPcAegNnZ2Zqbmxt4XgsLC/z5F78/8H7L4bF3z01k3IWFBYZ5rFaq1vqFdnue1O/ypNx48QmuOU3/ziO9eyfJmfQC/+NV9amu/FSSdd3964DjXf0YcGHf7uuBJ0YZX5I0mFHevRPgNuBwVX247679wI5ueQdwZ199e5KzkmwENgH3DTu+JGlwo1zeuQx4L/Bgkge62h8CNwP7klwHPA5cDVBVh5LsAx6m986f66uqrQt1kjRhQ4d+VX2Rpa/TA1x+in12A7uHHVOSNBo/kStJDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktSQkf9ylqR2bNh119jHvPHiExhVy8czfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1Jaogfc5OGNKlPp147gXE1PTzTl6SGGPqS1BBDX5IaYuhLUkN8IVcr3iReUJVWKs/0Jakhhr4kNcTQl6SGjP2afpKtwEeAVcBHq+rmcc9hmk3q+vZjN185kXElDWasZ/pJVgF/BbwN2Ay8K8nmcc5Bklo27jP9S4GjVfUNgCTzwDbg4THP47Sa1Nn2JP+AtF9JIK0MqarxDZb8OrC1qn6rW38v8ItV9b6TttsJ7OxWXw8cGWK484BvjzDdlai1nlvrF+y5FcvR809V1etOLo77tDBL1H7sWaeq9gB7Rhooub+qZkc5xkrTWs+t9Qv23IrT2fO4371zDLiwb3098MSY5yBJzRp36P8zsCnJxiSvALYD+8c8B0lq1lgv71TViSTvA/6R3ls2P1ZVh07TcCNdHlqhWuu5tX7Bnltx2noe6wu5kqTJ8hO5ktQQQ1+SGjJ1oZ9ka5IjSY4m2TXp+SyXJB9LcjzJQ321tUnuSfJId7um776busfgSJIrJjPr0SS5MMnnkxxOcijJDV19KvtO8sok9yX5atfvH3f1qey3X5JVSb6S5DPd+lT3nOSxJA8meSDJ/V1tPD1X1dT80Htx+OvATwOvAL4KbJ70vJapt7cAbwIe6qv9GbCrW94F/Gm3vLnr/SxgY/eYrJp0D0P0vA54U7f8GuBfu96msm96n2NZ3S2fCXwJePO09ntS778H/B3wmW59qnsGHgPOO6k2lp6n7Uz/f7/moap+APzoax5WvKr6AvCdk8rbgL3d8l7gqr76fFU9V1WPAkfpPTYrSlU9WVVf7pa/BxwGLmBK+66exW71zO6nmNJ+fyTJeuBK4KN95anu+RTG0vO0hf4FwDf71o91tWk1U1VPQi8ggfO7+tQ9Dkk2AL9A7+x3avvuLnM8ABwH7qmqqe638xfAHwD/3Veb9p4L+GySg93XzsCYep62P5f4kr7moQFT9TgkWQ18EvhAVX03Waq93qZL1FZU31X1PPDGJOcCn07yhhfYfMX3m+QdwPGqOphk7qXsskRtRfXcuayqnkhyPnBPkq+9wLbL2vO0nem39jUPTyVZB9DdHu/qU/M4JDmTXuB/vKo+1ZWnvu+qegZYALYy3f1eBrwzyWP0Lsf+apLbme6eqaonutvjwKfpXa4ZS8/TFvqtfc3DfmBHt7wDuLOvvj3JWUk2ApuA+yYwv5Gkd0p/G3C4qj7cd9dU9p3kdd0ZPknOBt4KfI0p7Regqm6qqvVVtYHe7+vnquo9THHPSV6d5DU/WgZ+DXiIcfU86VexT8Or4m+n9y6PrwMfnPR8lrGvTwBPAj+k98x/HfCTwAHgke52bd/2H+wegyPA2yY9/yF7/hV6/439F+CB7uft09o38HPAV7p+HwL+qKtPZb9L9D/H/717Z2p7pvfuwq92P4d+lFPj6tmvYZCkhkzb5R1J0gsw9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JD/gcdR5+RDOPXMgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(shortened, columns = ['text'])\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "df.n_tokens.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content is now broken down into smaller chunks and a simple request can be sent to the OpenAI API specifying the use of the new text-embedding-ada-002 model to create the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>research?authors=arthur petron.</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.004733280744403601, -0.006547232624143362,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blog dall e 2 update#OpenAI.  DALL·E 2 researc...</td>\n",
       "      <td>491</td>\n",
       "      <td>[-0.020911892876029015, -0.016629071906208992,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>research?authors=amanda askell.</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.0025514927692711353, -0.015087689273059368,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>research?authors=kenny hsu.</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.01220623031258583, 0.0003468284267000854, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>careers software engineer platform reliability...</td>\n",
       "      <td>406</td>\n",
       "      <td>[0.010812606662511826, -0.032961901277303696, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  n_tokens  \\\n",
       "0                    research?authors=arthur petron.         9   \n",
       "1  blog dall e 2 update#OpenAI.  DALL·E 2 researc...       491   \n",
       "2                    research?authors=amanda askell.         9   \n",
       "3                        research?authors=kenny hsu.         8   \n",
       "4  careers software engineer platform reliability...       406   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.004733280744403601, -0.006547232624143362,...  \n",
       "1  [-0.020911892876029015, -0.016629071906208992,...  \n",
       "2  [0.0025514927692711353, -0.015087689273059368,...  \n",
       "3  [0.01220623031258583, 0.0003468284267000854, 0...  \n",
       "4  [0.010812606662511826, -0.032961901277303696, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n",
    "\n",
    "df.to_csv('processed/embeddings.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Q&A System with Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the embeddings into a NumPy array is the first step, which will provide more flexibility in how to use it given the many functions available that operate on NumPy arrays. It will also flatten the dimension to 1-D, which is the required format for many subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>research?authors=arthur petron.</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.004733280744403601, -0.006547232624143362,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blog dall e 2 update#OpenAI.  DALL·E 2 researc...</td>\n",
       "      <td>491</td>\n",
       "      <td>[-0.020911892876029015, -0.016629071906208992,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>research?authors=amanda askell.</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.0025514927692711353, -0.015087689273059368,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>research?authors=kenny hsu.</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.01220623031258583, 0.0003468284267000854, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>careers software engineer platform reliability...</td>\n",
       "      <td>406</td>\n",
       "      <td>[0.010812606662511826, -0.032961901277303696, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  n_tokens  \\\n",
       "0                    research?authors=arthur petron.         9   \n",
       "1  blog dall e 2 update#OpenAI.  DALL·E 2 researc...       491   \n",
       "2                    research?authors=amanda askell.         9   \n",
       "3                        research?authors=kenny hsu.         8   \n",
       "4  careers software engineer platform reliability...       406   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.004733280744403601, -0.006547232624143362,...  \n",
       "1  [-0.020911892876029015, -0.016629071906208992,...  \n",
       "2  [0.0025514927692711353, -0.015087689273059368,...  \n",
       "3  [0.01220623031258583, 0.0003468284267000854, 0...  \n",
       "4  [0.010812606662511826, -0.032961901277303696, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from openai.embeddings_utils import distances_from_embeddings\n",
    "\n",
    "df=pd.read_csv('processed/embeddings.csv', index_col=0)\n",
    "df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question needs to be converted to an embedding with a simple function, now that the data is ready. This is important because the search with embeddings compares the vector of numbers (which was the conversion of the raw text) using cosine distance. The vectors are likely related and might be the answer to the question if they are close in cosine distance. The OpenAI python package has a built in distances_from_embeddings function which is useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(\n",
    "    question, df, max_len=1800, size=\"ada\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a context for a question by finding the most similar context from the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the embeddings for the question\n",
    "    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
    "\n",
    "    # Get the distances from the embeddings\n",
    "    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n",
    "\n",
    "\n",
    "    returns = []\n",
    "    cur_len = 0\n",
    "\n",
    "    # Sort by distance and add the text to the context until the context is too long\n",
    "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
    "        \n",
    "        # Add the length of the text to the current length\n",
    "        cur_len += row['n_tokens'] + 4\n",
    "        \n",
    "        # If the context is too long, break\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "        \n",
    "        # Else add it to the text that is being returned\n",
    "        returns.append(row[\"text\"])\n",
    "\n",
    "    # Return the context\n",
    "    return \"\\n\\n###\\n\\n\".join(returns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text was broken up into smaller sets of tokens, so looping through in ascending order and continuing to add the text is a critical step to ensure a full answer. The max_len can also be modified to something smaller, if more content than desired is returned.\n",
    "\n",
    "The previous step only retrieved chunks of texts that are semantically related to the question, so they might contain the answer, but there's no guarantee of it. The chance of finding an answer can be further increased by returning the top 5 most likely results.\n",
    "\n",
    "The answering prompt will then try to extract the relevant facts from the retrieved contexts, in order to formulate a coherent answer. If there is no relevant answer, the prompt will return “I don’t know”.\n",
    "\n",
    "A realistic sounding answer to the question can be created with the completion endpoint using text-davinci-003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    df,\n",
    "    model=\"text-davinci-003\",\n",
    "    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n",
    "    max_len=1800,\n",
    "    size=\"ada\",\n",
    "    debug=False,\n",
    "    max_tokens=150,\n",
    "    stop_sequence=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question based on the most similar context from the dataframe texts\n",
    "    \"\"\"\n",
    "    context = create_context(\n",
    "        question,\n",
    "        df,\n",
    "        max_len=max_len,\n",
    "        size=size,\n",
    "    )\n",
    "    # If debug, print the raw model response\n",
    "    if debug:\n",
    "        print(\"Context:\\n\" + context)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        # Create a completions using the question and context\n",
    "        response = openai.Completion.create(\n",
    "            prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=stop_sequence,\n",
    "            model=model,\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What day is it?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT-4.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is the latest OPENAI model?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greg Brockman and Ilya Sutskever founded OpenAI.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Who founded OPENAI?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jukebox is a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is Jukebox?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DALL·E is an AI system that creates realistic images and art from a description in natural language.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is Dall-E?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Can you fine-tune ChatGPT?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can customize GPT-3 for your application with one command and use it immediately in our API: openai api fine_tunes.create -t.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is the best way to fine-tune GPT-3?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, OpenAI does not store model outputs. Data submitted by the user through the Files endpoint, for instance to fine-tune a model, is retained until the user deletes the file.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Does OPENAI store my model outputs?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChatGPT was released on November 30, 2022.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"When was ChatGPT released?\", debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
